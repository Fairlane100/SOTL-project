---
title: "SOTL-withStops and Equivs"
author: "Ebner"
date: "May 27, 2016"
output: word_document
---


```{r, echo=F, warning=F, message=F}
library(mallet)
#library(wordcloud)
#library(reshape2)
#library("Matrix")
#library(SnowballC) stemmer was kind of shitty
#library(hunspell) stemmer better and worse, ergo, still shitty
library(rmarkdown)
library(qdap)
library(tm)
source("SOTLutils.R")

NUMTOPICS <- 13
```

Read input abstract data, stopwords, and equivalencies.  
Remove words that are not useful to the analysis.  
Remove abstracts that are too short (or empty).  
Print statistics about the word counts of remaining abstracts.  
```{r}
# read in csv files, 
#orig <- read.csv("Combined clean  5-31-16.csv", 
orig <- read.csv("Combined clean  6-13-16.csv", 
                 stringsAsFactors = FALSE)
SOTLstopwords <- read.csv("SOTL-stoplist.csv", 
                          stringsAsFactors = FALSE, header = F)$V1
SOTLequivalentWords <- read.csv("SOTL-equivalencies.csv", 
                                stringsAsFactors = FALSE, header = F)
SOTLPhrasesTerms <- read.csv("SOTL-termConcat.csv", 
                                stringsAsFactors = FALSE, header = F)
# defaulted to data.frame.  needs to be in vector form.
SOTLPhrasesTerms <- as.vector(as.matrix(SOTLPhrasesTerms))
# keep the original for comparison
c <- orig
# Clean the abstract data before processing
c$abstract <- cleanText(c$abstract, 
                        SOTLstopwords, 
                        SOTLequivalentWords,
                        SOTLPhrasesTerms)
# remove abstracts which are < 8 words (including empty ones)
rowsToRemove <- getBadRows(c$abstract, 8)
c <- c[-rowsToRemove,]

# word count statistics of remaining abstracts
abstractWords <- unlist(lapply(strsplit(c$abstract, "\\s+"), length))
summary(abstractWords)
```
The original input abstract count = `r length(orig$abstract)`.  
Cleaned abstract count = `r length(c$abstract)`.  

# Create word frequency tables
Create a table that has same structure as topic model output:  
 `r NUMTOPICS` rows (each row a topic) x lots cols (each col a word).  
The value of matrix is word count of topic.  
```{r}
# create a df with same format as topic.words.df
z <- split(c$abstract,c$Disc)
# treat disciplines as 'document'
# dont collapse if want to treat abstracts like docs instead of disciplines.
z <- lapply(z, paste, collapse = " ") 
corpus <- VCorpus(VectorSource(z))
dtm <- DocumentTermMatrix(corpus)

disc.words.m <- as.matrix(dtm)
disc.words.df <- as.data.frame(disc.words.m)
```


Apply Topic Modeling to automatically cluster abstracts into sets.
```{r}
topout <- trainTopicModel(c, NUMTOPICS)
topic.words.df <- topout[[1]]
doc.topics <- round(topout[[2]])
doc.topics <- doc.topics/rowSums(doc.topics)
doc.topics[doc.topics < .1] <- 0
doc.topics <- doc.topics/rowSums(doc.topics)
doc.topics <- signif(doc.topics,digits=2)
topnames <- NULL
for (i in 1:NUMTOPICS) {
    topnames <- c(topnames,paste("Topic",i))
}
colnames(doc.topics) <- topnames

d <- cbind(as.data.frame(doc.topics),c)
write.csv(d, "topicsAndDisciplinesOut.csv")
```

Print frequent terms from each corpus.  
```{r}
# the 5 is only for print, the freqs returned contains all words
topic.freqs <- printFrequentTerms(topic.words.df,5)
disc.freqs  <- printFrequentTerms(disc.words.df,5)
```

Find intersection of top terms in each set.  
```{r}
topicsWithTopics.l <- getIntersectionMatrix(topic.freqs, topic.freqs, 100, NUMTOPICS)
intsectMatTopics <- topicsWithTopics.l[[1]]
discsWithDiscs.l <- getIntersectionMatrix(disc.freqs, disc.freqs, 100, NUMTOPICS)
intsectMatDiscs <- discsWithDiscs.l[[1]]
intFreq.l <- getIntersectionMatrix(topic.freqs, disc.freqs, 100, NUMTOPICS)
intersectionMatrix <- intFreq.l[[1]]
commonTerms <- intFreq.l[[2]]

# draw a heatmap of the intersection matrix
intscaled <- as.matrix(scale(intersectionMatrix))
heatmap(intscaled, Colv=NA,scale='none')
```

The Intersection matrix has `r NUMTOPICS` rows (topics) and `r NUMTOPICS` columns (disciplines).  
```{r}
# Common words within topics
print(intsectMatTopics)
# Common words within disciplines
print(intsectMatDiscs)
# Common words between topics and disciplines
print(intersectionMatrix)
```
Terms common to topics and disciplines are in the following list:  
```{r}
print(commonTerms)
```

```{r, echo=F}
# # create a list of 11 word sequences which is words from each of disciplines
# z <- split(c$abstract,c$Disc)
# # treat disciplines as 'document'
# # dont collapse if want to treat abstracts like docs instead of disciplines.
# z <- lapply(z, paste, collapse = " ") 
# # make term frequency matrix
# 
# corpus <- VCorpus(VectorSource(z))
# dtm <- DocumentTermMatrix(corpus)
# a.dtm1 <- DocumentTermMatrix(corpus, control = 
#                                  list(weighting =
#                                           function(x)
#                                               weightTfIdf(x, normalize =
#                                                               FALSE))) 
# #str(a.dtm1)
# a.dtm1 <- removeSparseTerms(a.dtm1, 0.4)
# #str(a.dtm1)
# 
# m <- as.matrix(a.dtm1)
# totalFrequentWords <- sort(colSums(m), decreasing=TRUE)
# freqs <- list()
# for(i in 1:length(unique(c$Disc))) {
#     tempF <- sort(m[i,], decreasing = TRUE)
#     freqs[[i]] <- tempF
#     # output top 5 from each
#     txt <- paste("Discipline ",i, " Top 5 words, and their word count")
#     writeLines(txt)
#     a <- freqs[[i]][1:5]
#     print(a)
#     writeLines("")
# }


```
