---
title: "SOTL-first pass"
author: "Ebner"
date: "May 27, 2016"
output: pdf_document
---

first try to do topic modeling with abstracts from several journals.
24 journals, 5625 non empty abstracts.
there are many parameters here to change the analysis.

Topic modeling requires that 'chunks' of text are input.
in this ananlysis, each abstract is a 'chunk'.
in the Text Analysis book, Jockers uses between 500 and 1000 words per chunk.

Number of topics is another factor.  

List of stop words another.

part of speech another.
```{r}
library(mallet)
library(wordcloud)
library(reshape2)
library("Matrix")
#library(SnowballC) stemmer was kind of shitty
#library(hunspell) stemmer better and worse, ergo, still shitty
library(tm)

NUMTOPICS <- 10
```
The input file here is "Combined clean  5-12-16.csv"
```{r}
# read in the whole csv file for processing
c <- read.csv("Combined clean  5-12-16.csv", stringsAsFactors = FALSE)
#dim(c)
# [1] 6775   27

# remove rows where abstract is NA
c <- c[which(c$abstract != ""),]
#dim(c)
# [1] 5625   27
s <- c
#s <- c[1:5,]
# split each abstract into words
ids.v <- paste(s$journal,s$year,s$Volume.number,s$Issue,s$page,sep="_")
ids.v <- gsub(" ","",ids.v)
words <- s$abstract
words.lower <- tolower(words)
words.nopunct <- gsub("[^[:alnum:][:space:]]", " ", words.lower)
words.nopunct <- words.nopunct[words.nopunct != ""]

# Split first to stem (this is done here becuase it also takes 's out)

# Make corpus dictionary from all words in all abstracts (after stop removal)
#using tm
words.stemmed <- NULL
# create the total corpus dictionary
dict <- unlist(strsplit(words.nopunct, "\\s+"))
dict <- removeWords(dict, stopwords("english"))
dict <- dict[dict != ""]
for (i in 1:length(words.nopunct)) {
    cat(".")
    corp <- unlist(strsplit(words.nopunct[[i]],"\\s+"))
    corp <- removeWords(corp, stopwords("english"))
    corp <- corp[corp != ""]

    stemmedCorpus <- stemDocument(corp)
    #stemCompletedCorpus <- stemCompletion(stemmedCorpus, dictionary = dict) #too slow
    stemCompletedCorpus <- stemmedCorpus
    stemCompletedCorpus <- stripWhitespace(stemCompletedCorpus)
    # collapse into single string
    stemCompletedCorpus <- paste(stemCompletedCorpus,sep="",collapse=" ")
    words.stemmed <- c(words.stemmed,stemCompletedCorpus)
}

# get rid of punctuation
words.l <- strsplit(words.stemmed, "\\s+")
# make into list of chr's, was list of array of chr's, where each was one word
chunks.v <- unlist(lapply(words.l, paste, collapse = " "))
#str(ids.v)
#str(chunks.v)
#summary(nchar(chunks.v))
```
summary info for the abstract lengths are:
```{r}
summary(nchar(chunks.v))
```

Create the model for training and topic modeling
```{r}
mallet.instances <- mallet.import(ids.v, chunks.v,
                                  "stoplist.csv",
                                  FALSE,
                                  token.regexp = "[\\p{L}]+")
topic.model <- MalletLDA(num.topics = NUMTOPICS)
topic.model$loadDocuments(mallet.instances)

vocabulary <- topic.model$getVocabulary()
length(vocabulary)
word.freqs <- mallet.word.freqs(topic.model = topic.model)
# top words in entire set
word.freqs <- word.freqs[ order(-word.freqs[,3], -word.freqs[,2]), ]
head(word.freqs,100)

```
Now Train the Model
```{r}
topic.model$train(400)

topic.words.m <- mallet.topic.words(topic.model,
                                    smoothed = TRUE,
                                    normalized = TRUE)
topic.words.notNormed.m <- mallet.topic.words(topic.model,
                                    smoothed = TRUE,
                                    normalized = FALSE)
topic.words.notNormed.m <- round(topic.words.notNormed.m)

dim(topic.words.m)
colnames(topic.words.m) <- vocabulary
colnames(topic.words.notNormed.m) <- vocabulary
#topic.words.m[,1:6]
# topic.words.notNormed.df is a data frame of
# 10 rows (topics) by 15963 columns (word 'types)
# the entries of row,col are the frequency of word in a given topic
topic.words.notNormed.df <- as.data.frame(topic.words.notNormed.m)

# top 3 words in each topic
mallet.topic.labels(topic.model,topic.words.m)
```
Show an array of plots that roughly show the words that are in each topic

```{r}
par( mfcol=c(2,5) ) 
#Sort rows by topic value.
topic.words.notNormed.sorted.df <- 
    topic.words.notNormed.df[, order(topic.words.notNormed.df[1,], topic.words.notNormed.df[2,], topic.words.notNormed.df[3,], topic.words.notNormed.df[4,], topic.words.notNormed.df[5,], topic.words.notNormed.df[6,], topic.words.notNormed.df[7,], topic.words.notNormed.df[8,], topic.words.notNormed.df[9,], topic.words.notNormed.df[10,])]
topic.words.notNormed.sorted.m <- as.matrix(topic.words.notNormed.sorted.df)
for (i in 1:NUMTOPICS) {
    nonzeros <- sum(topic.words.notNormed.sorted.df[i,]!=0)
    print(paste(nonzeros,"Unique words in topic",i))
    title <- paste("Topic",i, "word dist")
    barplot(topic.words.notNormed.sorted.m[i,],horiz = TRUE)
    title(main=title)
}

```
Create a word cloud for each topic found in the text.
also print out word frequencies

```{r}
for (i in 1:NUMTOPICS) {
    topic.top.words <- mallet.top.words(topic.model,
                                    topic.words.m[i,], 50)
    topic.top.words.notNormed <- mallet.top.words(topic.model,
                                    topic.words.notNormed.m[i,], 50)
    topic.top.words.notNormed$weights <-
        as.integer(topic.top.words.notNormed$weights)
    print(paste("TOPIC ",i))
    print(topic.top.words.notNormed)
    wordcloud(topic.top.words$words,
          topic.top.words$weights,random.order = F,
          c(4,0.2))
}
```
Topic Coherence shows the probability of a given 'topic' in a given abstract
Rows are abstracts, and columns are topics.  Since there are > 5000 abstracts we want to show this in a more digestible way by showing average topic probability based on journal name.(could be other, e.g. discipline)

```{r}
#list journals:
u <- unique(s$journal)
print(u)

doc.topics.m <- mallet.doc.topics(topic.model,
                                  smoothed = T,
                                  normalized = T)
doc.topics.df <- as.data.frame(doc.topics.m)
#5625 abstracts x 10 topics
journals <- s$journal
doc.topics.df <- cbind(journals,doc.topics.df)
doc.topics.mean.df <- aggregate(doc.topics.df[, 2:ncol(doc.topics.df)],
                                list(doc.topics.df[,1]),
                                     mean)
for (i in 1:NUMTOPICS) {
    title <- paste("Topic",i, "means across journals")
    colVal <- paste("V",i,sep = "")
    barplot(doc.topics.mean.df[,colVal],names.arg = c(1:nrow(doc.topics.mean.df)))
    title(main = title)       
}
```
Co-occurence matrix creation. 

```{r}
# convert the wide style to long style
idx <- data.frame(1:NUMTOPICS)
colnames(idx) <- "topic"
topic.words.notNormed.df <- cbind(idx,topic.words.notNormed.df)
topic.words.long.df <- melt(topic.words.notNormed.df, id=c("topic"))
dim(topic.words.long.df) 
dim(topic.words.long.df)
# sort topics x words matrix to show what words belong to which topics.
topic.words.sorted <- topic.words.long.df[ order(topic.words.long.df[,1]), ]

```

Calculate cooccurance matrix as t(m) * m, where m is 15k words wide, and 10 topics high.  the resultant matrix is 15kx15k and holds number of cooccurances.  
```{r}
# topic.words.long.df is a long form matrix with topic, word, value
# First Remove all 0s and 1s from topic.words.long.df
# because if there is only one occurance of a word, it cannot co-occur!
#topic.words.long.df <- topic.words.long.df[topic.words.long.df$value > 1,]
topic.words.long.df <- topic.words.long.df[topic.words.long.df$value != 0,]
topic.words.long.df <- topic.words.long.df[topic.words.long.df$value != 1,]
topic.fac <- topic.words.long.df[,1]
words.fac <- topic.words.long.df[,2]
sparseM <- sparseMatrix(
        as.numeric(topic.fac), 
        as.numeric(words.fac),
        dimnames = list(
                as.character(levels(topic.fac)), 
                as.character(levels(words.fac))),
        x = 1)
# calculating co-occurrences
v <- t(sparseM) %*% sparseM
# setting diagonal to zero
diag(v) <- 0

# Now lets take a look at it
summ <- summary(v)
summLU <- summ[summ$i > summ$j,]
cooccur.df <- data.frame(word1 = rownames(v)[summLU$i],
                        word2  = colnames(v)[summLU$j],
                        value  = summLU$x)
cooccur.df <- cooccur.df[cooccur.df$value != 0,]
cooccur.df <- cooccur.df[cooccur.df$value != 1,]
# now sort big to small
cooccur.df <- cooccur.df[ order(-cooccur.df[,3], cooccur.df[,1], cooccur.df[,2]), ]
hist(cooccur.df$value)
```


